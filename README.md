# Flash Attention

_Work in progress..._

Implementation of Flash Attention for self-attention in Triton. Supports both the forward and backward passes for causal and non-causal cases.


Other ideas:
- Multi-Head Latent Flash Attention
- GQA Flash Attention
- Native Sparse Attention with Flash Attention