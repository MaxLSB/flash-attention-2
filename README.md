# Flash Attention

_Work in progress..._

Implementation of Flash Attention for self-attention in Triton. 

Currently supporting:
- Forward pass
- Backward passe
- Causal Attention
- Global Attention
- Sliding Window Attention !!!


<!-- Other ideas:
- Multi-Head Latent Flash Attention
- GQA Flash Attention
- Native Sparse Attention with Flash Attention -->