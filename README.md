# Flash Attention

_Work in progress..._

Implementation of Flash Attention for self-attention in Triton. Supports both the forward and backward passes for causal and non-causal cases.
